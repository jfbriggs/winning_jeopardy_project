{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Winning Jeopardy\n",
    "\n",
    "Jeopardy is a popular TV show in the US where participants answer questions to win money. It's been running for a few decades, and is a major force in popular culture.\n",
    "\n",
    "Scenario: Let's say you want to compete on Jeopardy, and you're looking for any edge you can get to win. In this project, we'll work with a dataset of Jeopardy questions to figure out some patterns in the questions that could help us win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jeopardy Questions\n",
    "\n",
    "The dataset is named `jeopardy.csv`, and contains 20000 rows from the beginning of a full dataset of Jeopardy questions, which can be downloaded [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file).\n",
    "\n",
    "The columns contained in this dataset are:\n",
    "\n",
    "- `Show Number` -- the Jeopardy episode number of the show this question was in.\n",
    "- `Air Date` -- the date the episode aired.\n",
    "- `Round` -- the round of Jeopardy that the question was asked in. Jeopardy has several rounds as each episode progresses.\n",
    "- `Category` -- the category of the question.\n",
    "- `Value` -- the number of dollars answering the question correctly is worth.\n",
    "- `Question` -- the text of the question.\n",
    "- `Answer` -- the text of the answer.\n",
    "\n",
    "Let's import the data and take a look at the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "jeopardy = pd.read_csv(\"jeopardy.csv\")\n",
    "jeopardy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
       "       ' Question', ' Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the column names by modifying them to a more standardized Python format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy.columns = jeopardy.columns.str.replace('^\\s', '').str.replace(' ', '_').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['show_number', 'air_date', 'round', 'category', 'value', 'question',\n",
       "       'answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Text\n",
    "\n",
    "Before we start doing analysis on the Jeopardy questions, we need to normalize all of the text columns (the `question` and `answer` columns); ensuring that we make all words lowercase and remove punctuation (so that, for example, `Don't` and `don't` aren't considered different words when we compare them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = s.lower() # convert to lowercase\n",
    "    s = re.sub(\"[^A-Za-z0-9\\s]\", \"\", s) # remove punctuation\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy[\"clean_question\"] = jeopardy[\"question\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    for the last 8 years of his life galileo was u...\n",
       "1    no 2 1912 olympian football star at carlisle i...\n",
       "2    the city of yuma in this state has a record av...\n",
       "3    in 1963 live on the art linkletter show this c...\n",
       "4    signer of the dec of indep framer of the const...\n",
       "Name: clean_question, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"clean_question\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy[\"clean_answer\"] = jeopardy[\"answer\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    copernicus\n",
       "1    jim thorpe\n",
       "2       arizona\n",
       "3     mcdonalds\n",
       "4    john adams\n",
       "Name: clean_answer, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"clean_answer\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Other Columns\n",
    "\n",
    "Now that we've normalized the text columns, there are also some other columns to normalize.\n",
    "\n",
    "The `Value` column should be numeric, to allow us to manipulate it more easily.  We'll need to remove the dollar sign from the beginning of each value and convert the column from text to numeric.\n",
    "\n",
    "Also, the `Air Date` column should be a datetime, not a string, to enable us to work with it more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(val):\n",
    "    val = re.sub(\"[^A-Za-z0-9\\s]\", \"\", val) # remove punctuation\n",
    "    try:\n",
    "        val = int(val)\n",
    "    except:\n",
    "        val = 0\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy[\"clean_value\"] = jeopardy[\"value\"].apply(normalize_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    200\n",
       "1    200\n",
       "2    200\n",
       "3    200\n",
       "4    200\n",
       "Name: clean_value, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"clean_value\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy[\"air_date\"] = pd.to_datetime(jeopardy[\"air_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2004-12-31\n",
       "1   2004-12-31\n",
       "2   2004-12-31\n",
       "3   2004-12-31\n",
       "4   2004-12-31\n",
       "Name: air_date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"air_date\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers in Questions\n",
    "\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study at all, it would be helpful to figure out two things:\n",
    "\n",
    "- How often the answer is deducible from the question.\n",
    "- How often new questions are repeats of older questions.\n",
    "\n",
    "We can answer the second question by seeing how often complex words (>6 characters) reoccur.  We can answer the first question by seeing how many times words in the answer also occur in the question.  We'll work on the first question now, and come back to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches(row):\n",
    "    split_answer = row[\"clean_answer\"].split(\" \")\n",
    "    split_question = row[\"clean_question\"].split(\" \")\n",
    "    match_count = 0\n",
    "    \n",
    "    if \"the\" in split_answer:\n",
    "        split_answer.remove(\"the\") # \"the\" serves no meaningful purpose for us, so we remove it when it's found\n",
    "    if len(split_answer) == 0:\n",
    "        return 0 # prevents 0 division later\n",
    "    \n",
    "    for word in split_answer:\n",
    "        if word in split_question:\n",
    "            match_count += 1\n",
    "            \n",
    "    return match_count / len(split_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy[\"answer_in_question\"] = jeopardy.apply(count_matches, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060493257069335914"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"answer_in_question\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We caculated a value, for each answer, of how many of that answer's words also appeared in the question. By then calculating the mean of the entire column, we see that, on average, a Jeopardy answer contains around 6% of the words in the question.\n",
    "\n",
    "This would suggest that we won't simply be able to figure out an answer by hearing the question; we'll likely have to study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recycled Questions / Words\n",
    "\n",
    "Let's say  we want to investigate how often new questions are repeats of older ones.  We can't completely answer this, because we only have about 10% of the full Jeopardy question dataset, but we can at least investigate.\n",
    "\n",
    "To do this, we'll iterate through questions by air date and build a set of words that have been used in prior questions.  As we go, we'll compare each new question's words to the set to see which have been used previously, and use this value as a proxy for re-use of questions.  We'll omit words with fewer than 6 characters in order to filter out words like `the` and `than` that are commonly used, but don't tell us a lot about a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6877843800700828"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = set([])\n",
    "\n",
    "jeopardy = jeopardy.sort_values(by=[\"air_date\"])\n",
    "\n",
    "for i, row in jeopardy.iterrows():\n",
    "    split_question = row[\"clean_question\"].split(\" \")\n",
    "    split_question = [word for word in split_question if len(word) > 5] # filter out words with fewer than 6 characters\n",
    "    match_count = 0\n",
    "    \n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count += 1\n",
    "            \n",
    "    for word in split_question:\n",
    "        terms_used.add(word)\n",
    "        \n",
    "    if len(split_question) > 0:\n",
    "        match_count /= len(split_question) # convert match_count to a fraction of the # of words in the question\n",
    "    \n",
    "    question_overlap.append(match_count)\n",
    "\n",
    "jeopardy[\"question_overlap\"] = question_overlap\n",
    "jeopardy[\"question_overlap\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that nearly 70% of words (6 characters or longer) existed in prior questions.  This doesn't tell us anything about matches of phrases, just single words, which makes this finding relatively insignificant; but it does signal that it would be worth investigating the potential reuse of questions further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Value vs. High-Value Questions\n",
    "\n",
    "Let's say we only want to study questions that, historically, are high-value vs. low-value.  This will help us earn more money when we're on Jeopardy.\n",
    "\n",
    "We can figure out which terms correspond to high-value questions using a chi-squared test.  We'll first split questions into two categories: Low value, where `Value` is <800, and High value, where `Value` is >800.\n",
    "\n",
    "We can then iterate through our `terms_used` set, and:\n",
    "\n",
    "- Find the number of low value questions the word occurs in.\n",
    "- Find the number of high value questions the word occurs in.\n",
    "- Find the percentage of questions the word occurs in.\n",
    "- Based on the percentage of questions the word occurs in, find expected counts.\n",
    "- Compute the chi squared value based on the expected counts and the observed counts for high and low value questions.\n",
    "\n",
    "We can then find the words with the biggest differences in usage between high- and low-value questions by selecting the words with the highest associated chi-squared values. For the take of time/simplicity, we'll just do it for a small sample for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (0, 1), (0, 1), (0, 1), (0, 3)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def determine_value(row):\n",
    "    if row[\"clean_value\"] > 800:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "jeopardy[\"high_value\"] = jeopardy.apply(determine_value, axis = 1)\n",
    "\n",
    "def low_high_counts(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    \n",
    "    for i, row in jeopardy.iterrows():\n",
    "        split_question = row[\"clean_question\"].split(\" \")\n",
    "        if word in split_question:\n",
    "            if row[\"high_value\"] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "                \n",
    "    return high_count, low_count\n",
    "\n",
    "observed_counts = []\n",
    "\n",
    "comparison_terms = list(terms_used)[:5]\n",
    "\n",
    "for term in comparison_terms:\n",
    "    observed_counts.append(low_high_counts(term))\n",
    "    \n",
    "observed_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found the observed counts for a few terms, we can compute the expected counts and the chi-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=1.205888538380652, pvalue=0.27214791766901714)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "import numpy as np\n",
    "\n",
    "high_value_count = jeopardy[jeopardy[\"high_value\"] == 1].shape[0]\n",
    "low_value_count = jeopardy[jeopardy[\"high_value\"] == 0].shape[0]\n",
    "\n",
    "chi_squared = []\n",
    "\n",
    "for counts in observed_counts:\n",
    "    total = sum(counts) # total number of questions the term appears in\n",
    "    total_prop = total / jeopardy.shape[0] # proportion of combined count relative to total # of rows in dataset\n",
    "    high_val_expected = total_prop * high_value_count\n",
    "    low_val_expected = total_prop * low_value_count\n",
    "    \n",
    "    observed = np.array([counts[0], counts[1]])\n",
    "    expected = np.array([high_val_expected, low_val_expected])\n",
    "    \n",
    "    chi_squared.append(chisquare(observed, expected)) # will append each term's chi-squared statistic and p-value to chi_squared list\n",
    "\n",
    "chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Potential Next Steps\n",
    "\n",
    "None of the terms had a significant difference in usage between high-value and low-value rows.  Also, the frequencies were all lower than 5, so the chi-squared test isn't that valid.  It would be better to run this test only with terms that have higher frequencies.\n",
    "\n",
    "Some potential next steps for this project would be:\n",
    "\n",
    "- Finding a better way to eliminate non-informative words than just  removing words that are less than 6 characters long.\n",
    "- Performing the chi-squared test across more terms to see what terms have larger differences.\n",
    "- Looking more into the `category` column to see if any interesting analysis can be done with it.\n",
    "- Using the whole Jeopardy dataset instead of the subset currently being used.\n",
    "- Using phrases instead of single words when seeing if there's overlap between questions.  Single words don't capture the whole context of the question well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
